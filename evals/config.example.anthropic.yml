# Evaluation Framework Configuration
# This configuration is shared across all evaluation runner scripts
# Example configuration for Anthropic Claude models

# API endpoint for the evaluation server
api_endpoint: "http://localhost:8080"

# Model configurations for running evaluations
# These models are sent to the agent for processing requests

main_model:
  provider: "anthropic"
  model_name: "claude-3-5-sonnet-20241022"
  api_key: "${ANTHROPIC_API_KEY}"

mini_model:
  provider: "anthropic"
  model_name: "claude-3-5-haiku-20241022"
  api_key: "${ANTHROPIC_API_KEY}"

nano_model:
  provider: "anthropic"
  model_name: "claude-3-5-haiku-20241022"
  api_key: "${ANTHROPIC_API_KEY}"

# Model configuration for judging evaluation responses
# This model is used locally to assess the quality of agent responses

judge_model:
  provider: "anthropic"
  model_name: "claude-3-5-sonnet-20241022"
  api_key: "${ANTHROPIC_API_KEY}"

# Execution settings

execution:
  # Default number of evaluations to run per script execution
  default_limit: 20

  # Timeout for API requests (seconds) - set to max for slow custom API
  timeout: 3600

  # Number of concurrent evaluation requests
  concurrent_requests: 1

  # Delay between requests (seconds)
  request_delay: 1

# Reporting settings

reporting:
  # Directory for storing evaluation reports
  reports_dir: "reports"

  # Report format
  format: "csv"

  # Include detailed judge reasoning in reports
  include_reasoning: true
